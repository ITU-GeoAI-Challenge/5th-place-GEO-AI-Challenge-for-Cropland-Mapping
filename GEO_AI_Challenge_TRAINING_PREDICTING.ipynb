{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhEgyjjxNe0lYI7HFG+kyc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlr9Rb-Rf9vc","executionInfo":{"status":"ok","timestamp":1696772995720,"user_tz":180,"elapsed":2357,"user":{"displayName":"Adrián Cal","userId":"08994606752174732637"}},"outputId":"a0aed49c-3102-4bab-e3ed-5f41aa93c775"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["# Mount Google Drive to the Colab environment for accessing/saving files directly to/from Drive.\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","source":["import pandas as pd\n","import lightgbm as lgb\n","from hyperopt import fmin, hp, STATUS_OK, tpe, Trials\n","import csv\n","import pickle\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","import os\n","import random"],"metadata":{"id":"qnNkDh8mhLwu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TRAINING\n","\n","# Setting the seed for reproducibility\n","\n","seed = 42\n","\n","np.random.seed(seed)\n","random.seed(seed)\n","rstate = np.random.default_rng(seed)\n","\n","# Set the working directory to the specified path\n","\n","# Change the current working directory to the specified path\n","\n","os.chdir(\"/content/gdrive/MyDrive\") # Change to your desired directory path\n","\n","# Define the relative path where data is stored\n","\n","path = \"GEO-AI Challenge for Cropland Mapping by ITU/\" # Change to your desired directory path\n","\n","# Define variable names for feature extraction\n","\n","variables = ['DATT1', 'IRECI', 'NBR2', 'NDRE', 'NDVI', 'NDWI']\n","\n","# Create expected column names\n","\n","exp_cols = [f\"{i:02d}_{suffix}\" for i in range(36) for suffix in variables]\n","\n","def preprocess_dataset(filename):\n","\n","    \"\"\"\n","    Load and preprocess the dataset.\n","    1. Reads the CSV file.\n","    2. Drops unwanted columns.\n","    3. Adds missing columns with NaN values.\n","\n","    Args:\n","    - filename (str): Name of the CSV file to read.\n","\n","    Returns:\n","    - DataFrame: Preprocessed data.\n","    \"\"\"\n","\n","    data = pd.read_csv(path + filename)\n","    data = data.drop(columns=[\"system:index\", \"COUNTRY\", \"ID\", \"Lat\", \"Lon\", \".geo\"])\n","\n","    # Identify and fill missing columns with NaN values\n","\n","    miss_cols = [col for col in exp_cols if col not in data.columns]\n","    for col in miss_cols:\n","        data[col] = np.nan\n","\n","    return data\n","\n","def order_columns(data):\n","\n","    \"\"\"\n","    Order columns of the DataFrame based on the predefined variables.\n","\n","    Args:\n","    - data (DataFrame): The input data.\n","\n","    Returns:\n","    - DataFrame: Data with columns reordered.\n","   \"\"\"\n","\n","    target = data[\"Target\"]\n","\n","    ordered_columns = []\n","    for variable in variables:\n","        variable_cols = sorted([col for col in data.columns if variable in col])\n","        ordered_columns.extend(variable_cols)\n","    data = data[ordered_columns]\n","    data[\"Target\"] = target\n","\n","    return data\n","\n","# Load and preprocess the Iran and Sudan dataset\n","\n","fail_iran_sudan = preprocess_dataset(\"S2_TRAIN_MULTI_INDICES_IRAN_SUDAN.csv\")\n","fail_iran_sudan = order_columns(fail_iran_sudan)\n","\n","# Load and preprocess the Afghanistan dataset\n","\n","fail_afghanistan = preprocess_dataset(\"S2_TRAIN_MULTI_INDICES_AFGHANISTAN.csv\")\n","fail_afghanistan = order_columns(fail_afghanistan)\n","\n","# Combine the two datasets\n","\n","fail = pd.concat([fail_iran_sudan, fail_afghanistan])\n","\n","# Function to create an output file for storing run results\n","\n","def outputs_run_fun():\n","\n","    output = path + \"OUTPUTS.CSV\"\n","\n","    open_output = open(output,\"w\",newline = \"\")\n","\n","    write_output = csv.writer(open_output)\n","\n","    write_output.writerow([\"ITERATION\",\"F1_SCORE_TRAIN\",\"F1_SCORE_TEST\",\"num_leaves\",\"learning_rate\",\"max_depth\",\"min_child_samples\",\n","                       \"colsample_bytree\",\"subsample\",\"n_estimators\",\"reg_alpha\",\"reg_lambda\",\"subsample_for_bin\"])\n","\n","    open_output.close()\n","\n","    return output\n","\n","# Define the hyperparameter search space\n","\n","space = {\n","    'num_leaves': hp.quniform('num_leaves', 5, 50,1),\n","    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n","    'max_depth': hp.quniform('max_depth', 2, 10,1),\n","    'min_child_samples': hp.quniform('min_child_samples', 5, 30,1),\n","    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 0.75),\n","    'subsample': hp.uniform('subsample', 0.1, 0.75),\n","    'n_estimators': hp.quniform('n_estimators', 10, 100,1),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\",0,1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\",0,1),\n","    \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\",10,50,1)\n","}\n","\n","# Define the objective function for hyperparameter optimization\n","\n","def objective_fun(params):\n","\n","    global iteration\n","\n","    iteration += 1\n","\n","    # Convert hyperparameters to their appropriate data types\n","\n","    params = {\n","        'num_leaves': int(params['num_leaves']),\n","        'learning_rate': params['learning_rate'],\n","        'max_depth': int(params['max_depth']),\n","        'min_child_samples': int(params['min_child_samples']),\n","        'colsample_bytree': params['colsample_bytree'],\n","        'subsample': params['subsample'],\n","        'n_estimators': int(params['n_estimators']),\n","        \"reg_alpha\": params[\"reg_alpha\"],\n","        \"reg_lambda\": params[\"reg_lambda\"],\n","        \"subsample_for_bin\": int(params[\"subsample_for_bin\"])\n","    }\n","\n","    # Initialize LightGBM model with the given parameters\n","\n","    model = lgb.LGBMClassifier(**params, random_state = seed, verbose = -1)\n","\n","    # Define a stratified K-fold cross-validation\n","\n","    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)\n","\n","    train_f1_scores = []\n","    test_f1_scores = []\n","\n","    # Perform cross-validation\n","\n","    for train_idx, test_idx in skf.split(fail.drop(\"Target\",axis=1), fail.Target):\n","\n","        x_train, y_train = fail.iloc[train_idx].drop(\"Target\", axis=1), fail.iloc[train_idx].Target\n","        x_test, y_test = fail.iloc[test_idx].drop(\"Target\", axis=1), fail.iloc[test_idx].Target\n","\n","        # Train the model\n","\n","        model.fit(x_train, y_train)\n","\n","        # Calculate F1-score for training set\n","\n","        train_predict = model.predict(x_train)\n","        train_f1_scores.append(f1_score(y_train, train_predict, average=\"weighted\"))\n","\n","        # Calculate F1-score for test set\n","\n","        test_predict = model.predict(x_test)\n","        test_f1_scores.append(f1_score(y_test, test_predict, average=\"weighted\"))\n","\n","    # Calculate average F1-scores for training and testing sets\n","\n","    avg_train_f1_score = sum(train_f1_scores) / len(train_f1_scores)\n","    avg_test_f1_score = sum(test_f1_scores) / len(test_f1_scores)\n","\n","    # Log the results and hyperparameters into a CSV file\n","\n","    open_output = open(output,\"a\",newline = \"\")\n","\n","    write_output = csv.writer(open_output)\n","\n","    write_output.writerow([iteration,avg_train_f1_score,avg_test_f1_score,params[\"num_leaves\"],params[\"learning_rate\"],params[\"max_depth\"],\n","                       params[\"min_child_samples\"],params[\"colsample_bytree\"],params[\"subsample\"],\n","                       params[\"n_estimators\"],params[\"reg_alpha\"],params[\"reg_lambda\"],params[\"subsample_for_bin\"]])\n","\n","    return {\"loss\":-avg_test_f1_score,\"status\":STATUS_OK}\n","\n","# Function to run hyperparameter optimization\n","\n","def runs_fun(evals_num):\n","\n","    global output\n","    global iteration\n","\n","    # Initialize output file to log results\n","\n","    output = outputs_run_fun()\n","\n","    iteration = 0\n","\n","    # Run hyperparameter optimization\n","\n","    best_params = fmin(fn = objective_fun,space = space,algo = tpe.suggest, max_evals = evals_num, trials = Trials(), rstate = rstate)\n","\n","    return best_params\n","\n","# Run the optimization for a given number of evaluations\n","\n","best_params = runs_fun(2000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaXa_BNVhaJV","executionInfo":{"status":"ok","timestamp":1696777194731,"user_tz":180,"elapsed":1288046,"user":{"displayName":"Adrián Cal","userId":"08994606752174732637"}},"outputId":"686fbf18-e0b1-4f78-8d53-990823bb6bab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100%|██████████| 2000/2000 [21:27<00:00,  1.55trial/s, best loss: -0.9393250724332699]\n"]}]},{"cell_type":"code","source":["# Convert some of the hyperparameters to integer type\n","\n","best_params[\"max_depth\"] = int(best_params[\"max_depth\"])\n","best_params[\"min_child_samples\"] = int(best_params[\"min_child_samples\"])\n","best_params[\"n_estimators\"] = int(best_params[\"n_estimators\"])\n","best_params[\"num_leaves\"] = int(best_params[\"num_leaves\"])\n","best_params[\"subsample_for_bin\"] = int(best_params[\"subsample_for_bin\"])\n","\n","# Initialize the best LightGBM model with optimized hyperparameters\n","\n","best_model = lgb.LGBMClassifier(**best_params, random_state = seed, verbose = -1)\n","\n","# Split the data into features and target variable\n","\n","x_train, y_train = fail.drop(\"Target\",axis=1), fail.Target\n","\n","# Train the best model on the entire dataset\n","\n","best_model.fit(x_train,y_train)\n","\n","# Save the trained model to a file\n","\n","with open(path+\"BEST_MODEL_MULTI_INDICE_LGBM.pkl\",\"wb\") as f:\n","    pickle.dump(best_model,f)"],"metadata":{"id":"zT-Hob2Sl4ck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PREDICTING\n","\n","def preprocess_data(path, filename):\n","\n","    \"\"\"Loads the dataset, drops unwanted columns, and orders the columns based on specified variables.\"\"\"\n","\n","    data = pd.read_csv(path + filename)\n","    ids = data.ID\n","    drop_cols = [\"system:index\", \"COUNTRY\", \"Lat\", \"Lon\", \".geo\", \"ID\"]\n","    data = data.drop(columns=drop_cols)\n","\n","    miss_cols = [col for col in exp_cols if col not in data.columns]\n","    for col in miss_cols:\n","        data[col] = np.nan\n","\n","    ordered_columns = [col for variable in variables for col in sorted(data.columns) if variable in col]\n","    data = data[ordered_columns]\n","\n","    return data, ids\n","\n","# Define columns and variables for preprocessing\n","\n","variables = ['DATT1', 'IRECI', 'NBR2', 'NDRE', 'NDVI', 'NDWI']\n","\n","exp_cols = [f\"{i:02d}_{suffix}\" for i in range(36) for suffix in variables]\n","\n","# Preprocess Iran-Sudan data\n","\n","fail_iran_sudan, IDs_iran_sudan = preprocess_data(path, \"S2_TEST_MULTI_INDICES_IRAN_SUDAN.csv\")\n","\n","iran_sudan_pred = pd.DataFrame(best_model.predict(fail_iran_sudan), columns=[\"Target\"])\n","\n","iran_sudan_pred[\"ID\"] = IDs_iran_sudan\n","\n","# Preprocess Afghanistan data\n","\n","fail_afghanistan, IDs_afghanistan = preprocess_data(path, \"S2_TEST_MULTI_INDICES_AFGHANISTAN.csv\")\n","\n","afghanistan_pred = pd.DataFrame(best_model.predict(fail_afghanistan), columns=[\"Target\"])\n","\n","afghanistan_pred[\"ID\"] = IDs_afghanistan\n","\n","# Concatenate predictions and merge with sample submission\n","\n","predictions = pd.concat([iran_sudan_pred, afghanistan_pred])\n","\n","submission = pd.read_csv(path + \"SampleSubmission.csv\")\n","\n","sub_merged = submission.merge(predictions, on=\"ID\", how=\"left\").drop(\"Target_x\", axis=1).rename(columns={\"Target_y\": \"Target\"})\n","\n","sub_merged.to_csv(path + \"Submission_MULTI_INDICES_LGBM_SKF.csv\", index=False)"],"metadata":{"id":"ZEyvEvbml-ZU"},"execution_count":null,"outputs":[]}]}